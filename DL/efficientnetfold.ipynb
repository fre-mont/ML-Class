{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":4276,"status":"ok","timestamp":1669507743800,"user":{"displayName":"서가연","userId":"09789171349553603896"},"user_tz":-540},"id":"BLTT6vL4mKNd"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import shutil\n","import glob\n","import seaborn as sns\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","from tensorflow.keras.preprocessing import image\n"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":3008,"status":"ok","timestamp":1669507766927,"user":{"displayName":"서가연","userId":"09789171349553603896"},"user_tz":-540},"id":"19Wi_hwmMPQA"},"outputs":[],"source":["train_df = pd.read_csv('train.csv')\n","test_df = pd.read_csv('submission.csv')\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1669507766927,"user":{"displayName":"서가연","userId":"09789171349553603896"},"user_tz":-540},"id":"DJcpPQI0nSEc","outputId":"46f0c8f8-1615-482a-de4a-137ecaf3bc96"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>google_pork_belly_89.jpg</td>\n","      <td>pork_belly</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>google_pasta_877.jpg</td>\n","      <td>pasta</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>google_janchi_guksu_211.jpg</td>\n","      <td>janchi_guksu</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>google_pizza_598.jpg</td>\n","      <td>pizza</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>naver_pasta_316.jpg</td>\n","      <td>pasta</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                         image         label\n","0     google_pork_belly_89.jpg    pork_belly\n","1         google_pasta_877.jpg         pasta\n","2  google_janchi_guksu_211.jpg  janchi_guksu\n","3         google_pizza_598.jpg         pizza\n","4          naver_pasta_316.jpg         pasta"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["train_df.head()  \n","\n","# image 경로와 label 존재 "]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1669507766928,"user":{"displayName":"서가연","userId":"09789171349553603896"},"user_tz":-540},"id":"vFgRyNXYuNZR","outputId":"cfb83a27-a15f-445b-85e0-a49bff2215a8"},"outputs":[{"data":{"text/plain":["array(['pork_belly', 'pasta', 'janchi_guksu', 'pizza', 'pork_cutlet',\n","       'cake', 'jajangmyeon', 'steak', 'ramen', 'tteokbokki',\n","       'grilled_eel', 'fried_chicken'], dtype=object)"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["train_df['label'].unique()  # 총 12개의 클래스 존재 "]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1669507766928,"user":{"displayName":"서가연","userId":"09789171349553603896"},"user_tz":-540},"id":"DPMl103CD5-5","outputId":"c9903b88-ddc1-4d61-9cfc-62801712cca2"},"outputs":[{"data":{"text/plain":["cake             1184\n","pasta            1072\n","steak            1048\n","fried_chicken    1034\n","pizza             963\n","grilled_eel       951\n","ramen             881\n","pork_cutlet       869\n","janchi_guksu      856\n","tteokbokki        841\n","jajangmyeon       833\n","pork_belly        789\n","Name: label, dtype: int64"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["train_df['label'].value_counts()"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":366,"status":"ok","timestamp":1669508590274,"user":{"displayName":"서가연","userId":"09789171349553603896"},"user_tz":-540},"id":"d07UtiDW3bUn"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","data1, data2 = train_test_split(train_df, test_size=0.2, stratify=train_df['label'])  # 80:20\n","data1, data3 = train_test_split(data1, test_size=0.25, stratify=data1['label'])       # 80 => 60 : 20 \n","data1, data4 = train_test_split(data1, test_size=0.35, stratify=data1['label'])       # 60 => 40 : 20\n","data1, data5 = train_test_split(data1, test_size=0.5, stratify=data1['label'])\n","\n","\n","\n","data1 = data1.reset_index(drop=True)\n","data2 = data2.reset_index(drop=True)\n","data3 = data3.reset_index(drop=True)\n","data4 = data4.reset_index(drop=True)\n","data5 = data5.reset_index(drop=True)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1669508590972,"user":{"displayName":"서가연","userId":"09789171349553603896"},"user_tz":-540},"id":"E8VfeCu6O5rG"},"outputs":[],"source":["path = os.path.join('train')\n","cls = lambda x : os.path.join(path, '_'.join(x.split('_')[1:-1])+'\\\\'+x)\n","\n","data1['image'] = data1['image'].map(lambda x : cls(x))\n","data2['image'] = data2['image'].map(lambda x : cls(x))\n","data3['image'] = data3['image'].map(lambda x : cls(x))\n","data4['image'] = data4['image'].map(lambda x : cls(x))\n","data5['image'] = data5['image'].map(lambda x : cls(x))\n"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":267,"status":"ok","timestamp":1669508943112,"user":{"displayName":"서가연","userId":"09789171349553603896"},"user_tz":-540},"id":"TYeb2SIMd5DS","outputId":"f7422395-a82d-4f68-9e06-8158d2349e5f"},"outputs":[{"name":"stdout","output_type":"stream","text":["(9114, 2)\n","(9056, 2)\n","(9057, 2)\n","(8943, 2)\n","(9114, 2)\n"]}],"source":["# train set \n","set1 = pd.concat([data2, data3, data4, data5], ignore_index=True) \n","set2 = pd.concat([data1, data3, data4, data5], ignore_index=True)\n","set3 = pd.concat([data1, data2, data4, data5], ignore_index=True)\n","set4 = pd.concat([data1, data2, data3, data5], ignore_index=True)\n","set5 = pd.concat([data1, data2, data3, data4], ignore_index=True)\n","\n","print(set1.shape)  \n","print(set2.shape)\n","print(set3.shape)\n","print(set4.shape)\n","print(set5.shape)"]},{"cell_type":"markdown","metadata":{"id":"vsetUmJeg6-5"},"source":["### Data aug"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":360,"status":"ok","timestamp":1669520960672,"user":{"displayName":"서가연","userId":"09789171349553603896"},"user_tz":-540},"id":"VhKmmj_afWv0"},"outputs":[],"source":["height, width, channel = (224, 224, 3)\n","labels = train_df.label.unique().tolist()\n","\n","\n","train_datagen= ImageDataGenerator(rescale=1./255,\n","                              rotation_range=30,\n","                              width_shift_range=0.2,\n","                              height_shift_range=0.2,\n","                              shear_range=0.2,\n","                              zoom_range=[0.8, 1],\n","                              horizontal_flip=True)\n","\n","valid_datagen = ImageDataGenerator(rescale=1./255)\n"]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1669520960928,"user":{"displayName":"서가연","userId":"09789171349553603896"},"user_tz":-540},"id":"2hpH1Ts0gUou"},"outputs":[],"source":["valid_list = [data1, data2, data3, data4, data5]\n","train_list = [set1, set2, set3, set4, set5]"]},{"cell_type":"markdown","metadata":{"id":"BBpVK4fSHOln"},"source":["# Modeling"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1669519595244,"user":{"displayName":"서가연","userId":"09789171349553603896"},"user_tz":-540},"id":"gtgwJqYHnXHG"},"outputs":[],"source":["from tensorflow.keras.applications import Xception\n","from tensorflow.keras.layers import Input, Dense, Activation, GlobalAveragePooling2D, Dropout, BatchNormalization, Flatten\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n","import tensorflow as tf \n"]},{"cell_type":"code","execution_count":36,"metadata":{"executionInfo":{"elapsed":265,"status":"ok","timestamp":1669518372546,"user":{"displayName":"서가연","userId":"09789171349553603896"},"user_tz":-540},"id":"cVvfnJzAL_oq"},"outputs":[],"source":["# 조기멈춤\n","stop = EarlyStopping(patience=5)\n","\n","# 자동 저장\n","checkpoint = ModelCheckpoint(\"Xceptionfold_model/model_weights\", monitor='val_accuracy',\n","                            save_weights_only=True, mode='max',verbose=2,save_best_only=True)\n","\n","reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n","                              patience=3, min_lr=0.0003)\n","\n","callbacks = [stop, checkpoint, reduce_lr]"]},{"cell_type":"code","execution_count":43,"metadata":{"executionInfo":{"elapsed":6983,"status":"ok","timestamp":1669519612807,"user":{"displayName":"서가연","userId":"09789171349553603896"},"user_tz":-540},"id":"-zWY3Vt8MRra"},"outputs":[],"source":["base_model = Xception(include_top=False, input_tensor=Input(shape=(224, 224, 3)), weights='imagenet', classes=12)\n","base_model.trainable = False   # 가중치 동결 후 학습 \n","\n","model = tf.keras.models.Sequential([\n","    base_model,\n","    GlobalAveragePooling2D(),\n","    BatchNormalization(),\n","    Dense(1024, activation='relu'),\n","    Dropout(0.2),\n","    Dense(512, activation='relu'),\n","    Dropout(0.2),\n","    Dense(256, activation='relu'),\n","    Dropout(0.2),\n","    Dense(128, activation='relu'),\n","    Dense(len(labels), activation='softmax'),  # output\n","\n","])\n"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9329309,"status":"ok","timestamp":1669530300383,"user":{"displayName":"서가연","userId":"09789171349553603896"},"user_tz":-540},"id":"_Yby3K_xhCbm","outputId":"4203aeeb-bf36-4fb6-da49-c1b5bc37429f"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","=========1번째 학습=========\n","\n","Found 9114 validated image filenames belonging to 12 classes.\n","Found 2207 validated image filenames belonging to 12 classes.\n","\n","======== train start! ========\n","\n","Epoch 1/10\n","  6/143 [>.............................] - ETA: 2:00 - loss: 2.3811 - accuracy: 0.2240"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\AI06\\anaconda3\\envs\\ml\\lib\\site-packages\\PIL\\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["143/143 [==============================] - ETA: 0s - loss: 1.0904 - accuracy: 0.6596\n","Epoch 1: val_accuracy improved from -inf to 0.77753, saving model to Xceptionfold_model\\model_weights\n","143/143 [==============================] - 138s 950ms/step - loss: 1.0904 - accuracy: 0.6596 - val_loss: 0.8524 - val_accuracy: 0.7775 - lr: 0.0010\n","Epoch 2/10\n","143/143 [==============================] - ETA: 0s - loss: 0.7875 - accuracy: 0.7575\n","Epoch 2: val_accuracy improved from 0.77753 to 0.79203, saving model to Xceptionfold_model\\model_weights\n","143/143 [==============================] - 135s 946ms/step - loss: 0.7875 - accuracy: 0.7575 - val_loss: 0.6673 - val_accuracy: 0.7920 - lr: 0.0010\n","Epoch 3/10\n","143/143 [==============================] - ETA: 0s - loss: 0.6908 - accuracy: 0.7860\n","Epoch 3: val_accuracy improved from 0.79203 to 0.80743, saving model to Xceptionfold_model\\model_weights\n","143/143 [==============================] - 135s 946ms/step - loss: 0.6908 - accuracy: 0.7860 - val_loss: 0.6183 - val_accuracy: 0.8074 - lr: 0.0010\n","Epoch 4/10\n","143/143 [==============================] - ETA: 0s - loss: 0.6370 - accuracy: 0.8060\n","Epoch 4: val_accuracy improved from 0.80743 to 0.81106, saving model to Xceptionfold_model\\model_weights\n","143/143 [==============================] - 136s 952ms/step - loss: 0.6370 - accuracy: 0.8060 - val_loss: 0.6005 - val_accuracy: 0.8111 - lr: 0.0010\n","Epoch 5/10\n","143/143 [==============================] - ETA: 0s - loss: 0.5966 - accuracy: 0.8152\n","Epoch 5: val_accuracy did not improve from 0.81106\n","143/143 [==============================] - 134s 940ms/step - loss: 0.5966 - accuracy: 0.8152 - val_loss: 0.5902 - val_accuracy: 0.8061 - lr: 0.0010\n","Epoch 6/10\n","143/143 [==============================] - ETA: 0s - loss: 0.5824 - accuracy: 0.8221\n","Epoch 6: val_accuracy did not improve from 0.81106\n","143/143 [==============================] - 134s 939ms/step - loss: 0.5824 - accuracy: 0.8221 - val_loss: 0.5925 - val_accuracy: 0.8097 - lr: 0.0010\n","Epoch 7/10\n","143/143 [==============================] - ETA: 0s - loss: 0.5358 - accuracy: 0.8349\n","Epoch 7: val_accuracy improved from 0.81106 to 0.82148, saving model to Xceptionfold_model\\model_weights\n","143/143 [==============================] - 135s 944ms/step - loss: 0.5358 - accuracy: 0.8349 - val_loss: 0.6249 - val_accuracy: 0.8215 - lr: 0.0010\n","Epoch 8/10\n","143/143 [==============================] - ETA: 0s - loss: 0.5260 - accuracy: 0.8370\n","Epoch 8: val_accuracy did not improve from 0.82148\n","143/143 [==============================] - 134s 935ms/step - loss: 0.5260 - accuracy: 0.8370 - val_loss: 0.5956 - val_accuracy: 0.8169 - lr: 0.0010\n","Epoch 9/10\n","143/143 [==============================] - ETA: 0s - loss: 0.4111 - accuracy: 0.8717\n","Epoch 9: val_accuracy improved from 0.82148 to 0.83416, saving model to Xceptionfold_model\\model_weights\n","143/143 [==============================] - 136s 954ms/step - loss: 0.4111 - accuracy: 0.8717 - val_loss: 0.5305 - val_accuracy: 0.8342 - lr: 3.0000e-04\n","Epoch 10/10\n","143/143 [==============================] - ETA: 0s - loss: 0.3697 - accuracy: 0.8810\n","Epoch 10: val_accuracy did not improve from 0.83416\n","143/143 [==============================] - 135s 939ms/step - loss: 0.3697 - accuracy: 0.8810 - val_loss: 0.5428 - val_accuracy: 0.8324 - lr: 3.0000e-04\n","\n","=========2번째 학습=========\n","\n","Found 9056 validated image filenames belonging to 12 classes.\n","Found 2265 validated image filenames belonging to 12 classes.\n","\n","======== train start! ========\n","\n","Epoch 1/10\n","  6/142 [>.............................] - ETA: 1:43 - loss: 0.4688 - accuracy: 0.8802"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\AI06\\anaconda3\\envs\\ml\\lib\\site-packages\\PIL\\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["142/142 [==============================] - ETA: 0s - loss: 0.5425 - accuracy: 0.8330\n","Epoch 1: val_accuracy improved from 0.83416 to 0.87594, saving model to Xceptionfold_model\\model_weights\n","142/142 [==============================] - 138s 958ms/step - loss: 0.5425 - accuracy: 0.8330 - val_loss: 0.3903 - val_accuracy: 0.8759 - lr: 0.0010\n","Epoch 2/10\n","142/142 [==============================] - ETA: 0s - loss: 0.5256 - accuracy: 0.8391\n","Epoch 2: val_accuracy improved from 0.87594 to 0.87770, saving model to Xceptionfold_model\\model_weights\n","142/142 [==============================] - 136s 955ms/step - loss: 0.5256 - accuracy: 0.8391 - val_loss: 0.3950 - val_accuracy: 0.8777 - lr: 0.0010\n","Epoch 3/10\n","142/142 [==============================] - ETA: 0s - loss: 0.4902 - accuracy: 0.8517\n","Epoch 3: val_accuracy did not improve from 0.87770\n","142/142 [==============================] - 135s 952ms/step - loss: 0.4902 - accuracy: 0.8517 - val_loss: 0.4588 - val_accuracy: 0.8640 - lr: 0.0010\n","Epoch 4/10\n","142/142 [==============================] - ETA: 0s - loss: 0.4878 - accuracy: 0.8499\n","Epoch 4: val_accuracy did not improve from 0.87770\n","142/142 [==============================] - 134s 942ms/step - loss: 0.4878 - accuracy: 0.8499 - val_loss: 0.4070 - val_accuracy: 0.8759 - lr: 0.0010\n","Epoch 5/10\n","142/142 [==============================] - ETA: 0s - loss: 0.3907 - accuracy: 0.8793\n","Epoch 5: val_accuracy improved from 0.87770 to 0.89934, saving model to Xceptionfold_model\\model_weights\n","142/142 [==============================] - 137s 961ms/step - loss: 0.3907 - accuracy: 0.8793 - val_loss: 0.3394 - val_accuracy: 0.8993 - lr: 3.0000e-04\n","Epoch 6/10\n","142/142 [==============================] - ETA: 0s - loss: 0.3420 - accuracy: 0.8933\n","Epoch 6: val_accuracy did not improve from 0.89934\n","142/142 [==============================] - 136s 958ms/step - loss: 0.3420 - accuracy: 0.8933 - val_loss: 0.3388 - val_accuracy: 0.8923 - lr: 3.0000e-04\n","Epoch 7/10\n","142/142 [==============================] - ETA: 0s - loss: 0.3189 - accuracy: 0.9003\n","Epoch 7: val_accuracy did not improve from 0.89934\n","142/142 [==============================] - 136s 955ms/step - loss: 0.3189 - accuracy: 0.9003 - val_loss: 0.3329 - val_accuracy: 0.8958 - lr: 3.0000e-04\n","Epoch 8/10\n","142/142 [==============================] - ETA: 0s - loss: 0.2923 - accuracy: 0.9043\n","Epoch 8: val_accuracy did not improve from 0.89934\n","142/142 [==============================] - 135s 953ms/step - loss: 0.2923 - accuracy: 0.9043 - val_loss: 0.3252 - val_accuracy: 0.8980 - lr: 3.0000e-04\n","Epoch 9/10\n","142/142 [==============================] - ETA: 0s - loss: 0.2948 - accuracy: 0.9065\n","Epoch 9: val_accuracy did not improve from 0.89934\n","142/142 [==============================] - 135s 951ms/step - loss: 0.2948 - accuracy: 0.9065 - val_loss: 0.3227 - val_accuracy: 0.8967 - lr: 3.0000e-04\n","Epoch 10/10\n","142/142 [==============================] - ETA: 0s - loss: 0.2743 - accuracy: 0.9129\n","Epoch 10: val_accuracy did not improve from 0.89934\n","142/142 [==============================] - 134s 946ms/step - loss: 0.2743 - accuracy: 0.9129 - val_loss: 0.3307 - val_accuracy: 0.8980 - lr: 3.0000e-04\n","\n","=========3번째 학습=========\n","\n","Found 9057 validated image filenames belonging to 12 classes.\n","Found 2264 validated image filenames belonging to 12 classes.\n","\n","======== train start! ========\n","\n","Epoch 1/10\n"," 12/142 [=>............................] - ETA: 1:44 - loss: 0.3074 - accuracy: 0.9062"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\AI06\\anaconda3\\envs\\ml\\lib\\site-packages\\PIL\\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["142/142 [==============================] - ETA: 0s - loss: 0.4300 - accuracy: 0.8672\n","Epoch 1: val_accuracy improved from 0.89934 to 0.91387, saving model to Xceptionfold_model\\model_weights\n","142/142 [==============================] - 138s 961ms/step - loss: 0.4300 - accuracy: 0.8672 - val_loss: 0.2438 - val_accuracy: 0.9139 - lr: 0.0010\n","Epoch 2/10\n","142/142 [==============================] - ETA: 0s - loss: 0.4109 - accuracy: 0.8707\n","Epoch 2: val_accuracy did not improve from 0.91387\n","142/142 [==============================] - 134s 945ms/step - loss: 0.4109 - accuracy: 0.8707 - val_loss: 0.2799 - val_accuracy: 0.9086 - lr: 0.0010\n","Epoch 3/10\n","142/142 [==============================] - ETA: 0s - loss: 0.4045 - accuracy: 0.8739\n","Epoch 3: val_accuracy did not improve from 0.91387\n","142/142 [==============================] - 134s 944ms/step - loss: 0.4045 - accuracy: 0.8739 - val_loss: 0.2920 - val_accuracy: 0.8966 - lr: 0.0010\n","Epoch 4/10\n","142/142 [==============================] - ETA: 0s - loss: 0.3849 - accuracy: 0.8821\n","Epoch 4: val_accuracy did not improve from 0.91387\n","142/142 [==============================] - 135s 948ms/step - loss: 0.3849 - accuracy: 0.8821 - val_loss: 0.3056 - val_accuracy: 0.8997 - lr: 0.0010\n","Epoch 5/10\n","142/142 [==============================] - ETA: 0s - loss: 0.3008 - accuracy: 0.9067\n","Epoch 5: val_accuracy did not improve from 0.91387\n","142/142 [==============================] - 134s 945ms/step - loss: 0.3008 - accuracy: 0.9067 - val_loss: 0.2590 - val_accuracy: 0.9077 - lr: 3.0000e-04\n","Epoch 6/10\n","142/142 [==============================] - ETA: 0s - loss: 0.2733 - accuracy: 0.9126\n","Epoch 6: val_accuracy did not improve from 0.91387\n","142/142 [==============================] - 134s 944ms/step - loss: 0.2733 - accuracy: 0.9126 - val_loss: 0.2543 - val_accuracy: 0.9134 - lr: 3.0000e-04\n","\n","=========4번째 학습=========\n","\n","Found 8943 validated image filenames belonging to 12 classes.\n","Found 2378 validated image filenames belonging to 12 classes.\n","\n","======== train start! ========\n","\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\AI06\\anaconda3\\envs\\ml\\lib\\site-packages\\PIL\\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","140/140 [==============================] - ETA: 0s - loss: 0.3958 - accuracy: 0.8852\n","Epoch 1: val_accuracy improved from 0.91387 to 0.92893, saving model to Xceptionfold_model\\model_weights\n","140/140 [==============================] - 138s 974ms/step - loss: 0.3958 - accuracy: 0.8852 - val_loss: 0.2334 - val_accuracy: 0.9289 - lr: 0.0010\n","Epoch 2/10\n","140/140 [==============================] - ETA: 0s - loss: 0.3602 - accuracy: 0.8884\n","Epoch 2: val_accuracy did not improve from 0.92893\n","140/140 [==============================] - 134s 959ms/step - loss: 0.3602 - accuracy: 0.8884 - val_loss: 0.2438 - val_accuracy: 0.9197 - lr: 0.0010\n","Epoch 3/10\n","140/140 [==============================] - ETA: 0s - loss: 0.3602 - accuracy: 0.8920\n","Epoch 3: val_accuracy did not improve from 0.92893\n","140/140 [==============================] - 134s 956ms/step - loss: 0.3602 - accuracy: 0.8920 - val_loss: 0.2526 - val_accuracy: 0.9247 - lr: 0.0010\n","Epoch 4/10\n","140/140 [==============================] - ETA: 0s - loss: 0.3390 - accuracy: 0.8949\n","Epoch 4: val_accuracy did not improve from 0.92893\n","140/140 [==============================] - 134s 953ms/step - loss: 0.3390 - accuracy: 0.8949 - val_loss: 0.2527 - val_accuracy: 0.9193 - lr: 0.0010\n","Epoch 5/10\n","140/140 [==============================] - ETA: 0s - loss: 0.2878 - accuracy: 0.9116\n","Epoch 5: val_accuracy did not improve from 0.92893\n","140/140 [==============================] - 133s 948ms/step - loss: 0.2878 - accuracy: 0.9116 - val_loss: 0.2144 - val_accuracy: 0.9277 - lr: 3.0000e-04\n","Epoch 6/10\n","140/140 [==============================] - ETA: 0s - loss: 0.2418 - accuracy: 0.9259\n","Epoch 6: val_accuracy improved from 0.92893 to 0.93103, saving model to Xceptionfold_model\\model_weights\n","140/140 [==============================] - 134s 956ms/step - loss: 0.2418 - accuracy: 0.9259 - val_loss: 0.2095 - val_accuracy: 0.9310 - lr: 3.0000e-04\n","Epoch 7/10\n","140/140 [==============================] - ETA: 0s - loss: 0.2308 - accuracy: 0.9272\n","Epoch 7: val_accuracy did not improve from 0.93103\n","140/140 [==============================] - 134s 961ms/step - loss: 0.2308 - accuracy: 0.9272 - val_loss: 0.2157 - val_accuracy: 0.9310 - lr: 3.0000e-04\n","Epoch 8/10\n","140/140 [==============================] - ETA: 0s - loss: 0.1928 - accuracy: 0.9372\n","Epoch 8: val_accuracy improved from 0.93103 to 0.93440, saving model to Xceptionfold_model\\model_weights\n","140/140 [==============================] - 135s 966ms/step - loss: 0.1928 - accuracy: 0.9372 - val_loss: 0.2057 - val_accuracy: 0.9344 - lr: 3.0000e-04\n","Epoch 9/10\n","140/140 [==============================] - ETA: 0s - loss: 0.2018 - accuracy: 0.9354\n","Epoch 9: val_accuracy did not improve from 0.93440\n","140/140 [==============================] - 134s 959ms/step - loss: 0.2018 - accuracy: 0.9354 - val_loss: 0.2026 - val_accuracy: 0.9315 - lr: 3.0000e-04\n","Epoch 10/10\n","140/140 [==============================] - ETA: 0s - loss: 0.1883 - accuracy: 0.9406\n","Epoch 10: val_accuracy did not improve from 0.93440\n","140/140 [==============================] - 134s 953ms/step - loss: 0.1883 - accuracy: 0.9406 - val_loss: 0.2082 - val_accuracy: 0.9340 - lr: 3.0000e-04\n","\n","=========5번째 학습=========\n","\n","Found 9114 validated image filenames belonging to 12 classes.\n","Found 2207 validated image filenames belonging to 12 classes.\n","\n","======== train start! ========\n","\n","Epoch 1/10\n"," 11/143 [=>............................] - ETA: 1:42 - loss: 0.2871 - accuracy: 0.9144"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\AI06\\anaconda3\\envs\\ml\\lib\\site-packages\\PIL\\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["143/143 [==============================] - ETA: 0s - loss: 0.3288 - accuracy: 0.9014\n","Epoch 1: val_accuracy improved from 0.93440 to 0.95242, saving model to Xceptionfold_model\\model_weights\n","143/143 [==============================] - 139s 959ms/step - loss: 0.3288 - accuracy: 0.9014 - val_loss: 0.1471 - val_accuracy: 0.9524 - lr: 0.0010\n","Epoch 2/10\n","143/143 [==============================] - ETA: 0s - loss: 0.3075 - accuracy: 0.9060\n","Epoch 2: val_accuracy did not improve from 0.95242\n","143/143 [==============================] - 134s 935ms/step - loss: 0.3075 - accuracy: 0.9060 - val_loss: 0.1864 - val_accuracy: 0.9429 - lr: 0.0010\n","Epoch 3/10\n","143/143 [==============================] - ETA: 0s - loss: 0.3001 - accuracy: 0.9066\n","Epoch 3: val_accuracy did not improve from 0.95242\n","143/143 [==============================] - 135s 942ms/step - loss: 0.3001 - accuracy: 0.9066 - val_loss: 0.2193 - val_accuracy: 0.9280 - lr: 0.0010\n","Epoch 4/10\n","143/143 [==============================] - ETA: 0s - loss: 0.2989 - accuracy: 0.9083\n","Epoch 4: val_accuracy did not improve from 0.95242\n","143/143 [==============================] - 134s 935ms/step - loss: 0.2989 - accuracy: 0.9083 - val_loss: 0.2632 - val_accuracy: 0.9216 - lr: 0.0010\n","Epoch 5/10\n","143/143 [==============================] - ETA: 0s - loss: 0.2406 - accuracy: 0.9278\n","Epoch 5: val_accuracy did not improve from 0.95242\n","143/143 [==============================] - 135s 941ms/step - loss: 0.2406 - accuracy: 0.9278 - val_loss: 0.2003 - val_accuracy: 0.9366 - lr: 3.0000e-04\n","Epoch 6/10\n","143/143 [==============================] - ETA: 0s - loss: 0.1906 - accuracy: 0.9379\n","Epoch 6: val_accuracy did not improve from 0.95242\n","143/143 [==============================] - 134s 939ms/step - loss: 0.1906 - accuracy: 0.9379 - val_loss: 0.2042 - val_accuracy: 0.9361 - lr: 3.0000e-04\n"]}],"source":["scores = []\n","best_model = []\n","\n","for iter in range(5):\n","    print(f\"\\n========={iter+1}번째 학습=========\\n\")\n","\n","    train_generator = train_datagen.flow_from_dataframe(train_list[iter],\n","                                                   x_col='image',\n","                                                   y_col='label',\n","                                                   batch_size=64,\n","                                                   color_mode= 'rgb',\n","                                                   target_size=(height, width), shuffle=True)\n","\n","    valid_generator = valid_datagen.flow_from_dataframe(valid_list[iter],\n","                                                   x_col='image',\n","                                                   y_col='label',\n","                                                   batch_size=64,\n","                                                   color_mode= 'rgb',\n","                                                   target_size=(height, width), shuffle=True) \n","    \n","    model.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","    print(\"\\n======== train start! ========\\n\")\n","    history = model.fit(train_generator, \n","                        epochs=10, \n","                        callbacks=callbacks,\n","                        validation_data=valid_generator)\n","\n","    best_model.append(model)\n","    scores.append(history.history['val_accuracy'])\n"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[],"source":["model.save(\"Xceptionfold.h5\")"]},{"cell_type":"markdown","metadata":{"id":"FTtXTLtOH_xl"},"source":["### Tuning"]},{"cell_type":"code","execution_count":47,"metadata":{"executionInfo":{"elapsed":265,"status":"ok","timestamp":1669531778439,"user":{"displayName":"서가연","userId":"09789171349553603896"},"user_tz":-540},"id":"HbMQm6Fz0wCl"},"outputs":[],"source":["tuned_scores = []\n","tuend_best_model = []\n","\n","for layer in model.layers[:-20]:\n","    layer.trainable = True"]},{"cell_type":"code","execution_count":51,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":197372,"status":"ok","timestamp":1669540323897,"user":{"displayName":"서가연","userId":"09789171349553603896"},"user_tz":-540},"id":"o3tEgxhAzIkf","outputId":"f8e14408-98db-4b6c-f698-59382a344ae7"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","=========1번째 학습=========\n","\n","Found 9114 validated image filenames belonging to 12 classes.\n","Found 2207 validated image filenames belonging to 12 classes.\n","\n","======== train start! ========\n","\n","Epoch 1/20\n"," 25/143 [====>.........................] - ETA: 1:30 - loss: 0.1897 - accuracy: 0.9385"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\AI06\\anaconda3\\envs\\ml\\lib\\site-packages\\PIL\\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["143/143 [==============================] - ETA: 0s - loss: 0.2225 - accuracy: 0.9334\n","Epoch 1: val_accuracy improved from 0.95242 to 0.96783, saving model to Xceptionfold_model\\model_weights\n","143/143 [==============================] - 139s 960ms/step - loss: 0.2225 - accuracy: 0.9334 - val_loss: 0.0977 - val_accuracy: 0.9678 - lr: 1.0000e-05\n","Epoch 2/20\n","101/143 [====================>.........] - ETA: 34s - loss: 0.2138 - accuracy: 0.9323"]}],"source":["for iter in range(5):\n","    print(f\"\\n========={iter+1}번째 학습=========\\n\")\n","\n","    train_generator = train_datagen.flow_from_dataframe(train_list[iter],\n","                                                        x_col='image',\n","                                                        y_col='label',\n","                                                        batch_size=64,\n","                                                        color_mode='rgb',\n","                                                        target_size=(height, width), shuffle=True)\n","\n","    valid_generator = valid_datagen.flow_from_dataframe(valid_list[iter],\n","                                                        x_col='image',\n","                                                        y_col='label',\n","                                                        batch_size=64,\n","                                                        color_mode='rgb',\n","                                                        target_size=(height, width), shuffle=True)\n","    \n","    model.compile(optimizer=Adam(0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","    print(\"\\n======== train start! ========\\n\")\n","    history = model.fit(train_generator, \n","                        epochs=20, \n","                        callbacks=callbacks,\n","                        validation_data=valid_generator)\n","\n","    best_model.append(model)\n","    scores.append(history.history['val_accuracy'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.save(\"Xception_tuned_fold.h5\")"]},{"cell_type":"markdown","metadata":{"id":"r9CUMDO7YkIb"},"source":["# Test "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["batch_size = len(test_df[\"image\"])\n","height, width, channel = 224, 224, 3\n","\n","path = os.path.join('test')\n","\n","test_image = np.zeros((batch_size, height, width, channel))\n","print(test_image.shape)\n","\n","cnt = 0\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for i in range(len(test_df[\"image\"])):\n","    path_img = test_df['image'][i]\n","    img = image.load_img(os.path.join(path, path_img),\n","                         target_size=(height, width))\n","\n","    img_tensor = image.img_to_array(img)\n","    img_tensor = np.array(img_tensor, dtype=\"float32\")\n","\n","    img_tensor /= 255\n","\n","    img_tensor = np.expand_dims(img_tensor, axis=0)\n","\n","    test_image[i] = img_tensor\n","\n","    if (i % 100 == 0):\n","        print(str(i) + \" 완료\")  # 진행 상황 확인용\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","pred2label = {}\n","for x in train_generator.class_indices.keys():\n","    pred2label[train_generator.class_indices[x]] = x\n","\n","\n","predictions = model.predict(test_image)\n","test_df['label'] = [pred2label[np.argmax(pred)] for pred in predictions]\n","\n","print(test_df)\n","test_df.to_csv('/submissions/Xception_5fold.csv', index=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPstOVDTGb2Zkxr5Enp10qT","machine_shape":"hm","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.8.13 64-bit ('ml': conda)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"vscode":{"interpreter":{"hash":"266dd27284d9605b7b690db531109e144e78033e56a499ac8e3cb0ba4280d983"}}},"nbformat":4,"nbformat_minor":0}
